{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "from config import db_password\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that takes in three arguments\n",
    "\n",
    "def etl_auto(wiki, kaggle, rating):\n",
    "    file_dir = 'C:/Users/shula/OneDrive/Desktop/UCB_Data_052020/UCB-202005-downloads/data'\n",
    "    \n",
    "    # Extract data from multiple sources \n",
    "    \n",
    "    try:\n",
    "        kaggle_metadata = pd.read_csv(f'{file_dir}/{kaggle}.csv', low_memory = False) \n",
    "        print('kaggle data converted')\n",
    "        ratings= pd.read_csv(f'{file_dir}/{rating}.csv', low_memory = False)       \n",
    "        print('rating data converted')\n",
    "        wiki_df = pd.read_csv(f'{file_dir}/{wiki}.csv', low_memory = False)   \n",
    "        print('wiki data converted')\n",
    "    except FileNotFoundError:    \n",
    "            with open(f'{file_dir}/{wiki}.json', mode='r') as f:\n",
    "                raw = json.load(f)\n",
    "            print('wiki data converted')\n",
    "            pass\n",
    "    \n",
    "    # Transform wiki data and convert into dataframe \n",
    "    # Clean duplicated columns and rename columns \n",
    "    wiki_movies = [movie for movie in raw\n",
    "                   if ('Director' in movie or 'Directed by' in movie) \n",
    "                   and 'imdb_link' in movie\n",
    "                   and 'No. of episodes' not in movie]\n",
    "    def clean_movie(movie):\n",
    "        alt_titles ={}\n",
    "        # Combine altenative titles into one list 'alt_titles' \n",
    "        for key in ['Also known as','Arabic','Cantonese','Chinese','French',\n",
    "                    'Hangul','Hebrew','Hepburn','Japanese','Literally',\n",
    "                    'Mandarin','McCune–Reischauer','Original title','Polish',\n",
    "                    'Revised Romanization','Romanized','Russian',\n",
    "                    'Simplified','Traditional','Yiddish']:\n",
    "            if key in movie:\n",
    "                alt_titles[key] = movie[key]\n",
    "                movie.pop(key)\n",
    "        if len(alt_titles) > 0:\n",
    "            movie['alt_titles'] = alt_titles\n",
    "        # Merge similar columns name\n",
    "        def change_column_name(old_name, new_name):\n",
    "            if old_name in movie:\n",
    "                movie[new_name] = movie.pop(old_name)\n",
    "        change_column_name('Adaptation by', 'Writer(s)')\n",
    "        change_column_name('Country of origin', 'Country')\n",
    "        change_column_name('Directed by', 'Director')\n",
    "        change_column_name('Distributed by', 'Distributor')\n",
    "        change_column_name('Edited by', 'Editor(s)')\n",
    "        change_column_name('Length', 'Running time')\n",
    "        change_column_name('Original release', 'Release date')\n",
    "        change_column_name('Music by', 'Composer(s)')\n",
    "        change_column_name('Produced by', 'Producer(s)')\n",
    "        change_column_name('Producer', 'Producer(s)')\n",
    "        change_column_name('Productioncompanies ', 'Production company(s)')\n",
    "        change_column_name('Productioncompany ', 'Production company(s)')\n",
    "        change_column_name('Released', 'Release Date')\n",
    "        change_column_name('Release Date', 'Release date')\n",
    "        change_column_name('Screen story by', 'Writer(s)')\n",
    "        change_column_name('Screenplay by', 'Writer(s)')\n",
    "        change_column_name('Story by', 'Writer(s)')\n",
    "        change_column_name('Theme music composer', 'Composer(s)')\n",
    "        change_column_name('Written by', 'Writer(s)')\n",
    "        # Return output\n",
    "        return movie\n",
    "    try:\n",
    "        clean_movies = [clean_movie(movie) for movie in wiki_movies]\n",
    "    except NameError:\n",
    "            pass\n",
    "    wiki_movies_df = pd.DataFrame(clean_movies)\n",
    "    # Substitute imdb_id with string extracted from imdb_link\n",
    "    wiki_movies_df['imdb_id'] = wiki_movies_df['imdb_link'].str.extract(r'(tt\\d{7})')\n",
    "    wiki_movies_df.drop_duplicates(subset='imdb_id', inplace=True)\n",
    "    # Catch columns that has more than 10% null  \n",
    "    not_null_col = [column for column in wiki_movies_df.columns if wiki_movies_df[column].isnull().sum() < len(wiki_movies_df) * 0.9]\n",
    "    wiki_movies_df = wiki_movies_df.loc[:,(not_null_col)]\n",
    "    # Drop na and convert list \n",
    "    box_office = wiki_movies_df['Box office'].dropna().apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "    budget = wiki_movies_df['Budget'].dropna().apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "    release_date = wiki_movies_df['Release date'].dropna().apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "    running_time = wiki_movies_df['Running time'].dropna().apply(lambda x: ' '.join(x) if type(x) == list else x)\n",
    "    # Parse box office values \n",
    "    form_one = r'\\$\\s*\\d+\\.?\\d*\\s*[mb]illi?on'\n",
    "    form_two = r'\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\s[mb]illion)'\n",
    "    box_office = box_office.str.replace(r'\\$.*[-—–](?![a-z])', '$', regex=True)\n",
    "    # Define function to convert box office value into float\n",
    "    def parse_dollars(s):\n",
    "        if type(s) != str:\n",
    "            return np.nan\n",
    "        if re.match(r'\\$\\s*\\d+\\.?\\d*\\s*milli?on', s, flags=re.IGNORECASE):\n",
    "            s = re.sub('\\$|\\s|[a-zA-Z]','',s)\n",
    "            value = float(s)*10**6\n",
    "            return(value)\n",
    "        elif re.match(r'\\$\\s*\\d+\\.?\\d*\\s*billi?on', s, flags=re.IGNORECASE):\n",
    "            s = re.sub('\\$|\\s|[a-zA-Z]','',s)\n",
    "            value = float(s)*10**9\n",
    "            return(value)\n",
    "        elif re.match(r'\\$\\s*\\d{1,3}(?:[,\\.]\\d{3})+(?!\\s[mb]illion)', s, flags=re.IGNORECASE):\n",
    "            s = re.sub('\\$|,','',s)\n",
    "            value = float(s)\n",
    "            return(value)\n",
    "        else:\n",
    "            return np.nan\n",
    "    wiki_movies_df['box_office'] = box_office.str.extract(f'({form_one}|{form_two})', flags=re.IGNORECASE)[0].apply(parse_dollars)\n",
    "    wiki_movies_df.drop('Box office', axis=1, inplace=True)\n",
    "    # Parse budget values\n",
    "    budget = budget.str.replace(r'\\$.*[-—–](?![a-z])','$',regex = True)\n",
    "    budget = budget.str.replace(r'\\[\\d+\\]\\s*', '')\n",
    "    wiki_movies_df['budget'] = budget.str.extract(f'({form_one}|{form_two})', flags=re.IGNORECASE)[0].apply(parse_dollars)\n",
    "    wiki_movies_df.drop('Budget', axis=1, inplace=True)\n",
    "    # Parse release date \n",
    "    date_form_one = r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s[123]\\d,\\s\\d{4}'\n",
    "    date_form_two = r'\\d{4}.[01]\\d.[123]\\d'\n",
    "    date_form_three = r'(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s\\d{4}'\n",
    "    date_form_four = r'\\d{4}'\n",
    "    wiki_movies_df['release_date'] = pd.to_datetime(release_date.str.extract(f'({date_form_one}|{date_form_two}|{date_form_three}|{date_form_four})')[0], infer_datetime_format=True)\n",
    "    # Parse running time\n",
    "    running_time[running_time.str.contains(r'^\\d*\\s*minutes$', flags=re.IGNORECASE) != True]\n",
    "    running_time[running_time.str.contains(r'^\\d*\\s*m', flags=re.IGNORECASE) != True]\n",
    "    running_time_extract = running_time.str.extract(r'(\\d+)\\s*ho?u?r?s?\\s*(\\d*)|(\\d+)\\s*m')\n",
    "    running_time_extract = running_time_extract.apply(lambda col: pd.to_numeric(col, errors='coerce')).fillna(0)\n",
    "    wiki_movies_df['running_time'] = running_time_extract.apply(lambda row: row[0]*60 + row[1] if row[2] == 0 else row[2], axis=1)\n",
    "    wiki_movies_df.drop('Running time', axis=1, inplace=True)\n",
    "    print('wiki data transformed')\n",
    "    \n",
    "    # Transform kaggle data\n",
    "    kaggle_metadata = kaggle_metadata[kaggle_metadata['adult'] == 'False'].drop('adult',axis='columns')\n",
    "    kaggle_metadata['video'] = kaggle_metadata['video'] == 'True'\n",
    "    kaggle_metadata['budget'] = kaggle_metadata['budget'].astype(int)\n",
    "    kaggle_metadata['id'] = pd.to_numeric(kaggle_metadata['id'], errors='raise')\n",
    "    kaggle_metadata['popularity'] = pd.to_numeric(kaggle_metadata['popularity'], errors='raise')\n",
    "    kaggle_metadata['release_date'] = pd.to_datetime(kaggle_metadata['release_date'])\n",
    "    print('kaggle data transformed')\n",
    "    \n",
    "    # Transform rating data \n",
    "    ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit='s')\n",
    "    rating_counts = ratings.groupby(['movieId','rating'], as_index=False).count() \\\n",
    "                .rename({'userId':'count'}, axis=1) \\\n",
    "                .pivot(index='movieId',columns='rating', values='count')\n",
    "    rating_counts.columns = ['rating_' + str(col) for col in rating_counts.columns]\n",
    "    print('rating data transformed')\n",
    "    \n",
    "    # Merge e data set \n",
    "    # Merge wiki and kaggle \n",
    "    movies_df = pd.merge(wiki_movies_df, kaggle_metadata, on='imdb_id', suffixes=['_wiki','_kaggle'])\n",
    "    try:\n",
    "        movies_df = movies_df.drop(movies_df[(movies_df['release_date_wiki'] > '1996-01-01') & (movies_df['release_date_kaggle'] < '1965-01-01')].index)\n",
    "    except KeyError:\n",
    "           pass \n",
    "    movies_df.drop(columns=['title_wiki','release_date_wiki','Language','Production company(s)'], inplace=True)\n",
    "    # fill missing kaggle data with wiki data \n",
    "    def fill_missing_kaggle_data(df, kaggle_column, wiki_column):\n",
    "        df[kaggle_column] = df.apply(lambda row: row[wiki_column] if row[kaggle_column] == 0 else row[kaggle_column], axis=1)\n",
    "        df.drop(columns=wiki_column, inplace=True)\n",
    "    fill_missing_kaggle_data(movies_df, 'runtime', 'running_time')\n",
    "    fill_missing_kaggle_data(movies_df, 'budget_kaggle', 'budget_wiki')\n",
    "    fill_missing_kaggle_data(movies_df, 'revenue', 'box_office')\n",
    "    # Remove useless columns\n",
    "    def drop_col(df):    \n",
    "        for col in df.columns:\n",
    "            lists_to_tuples = lambda x: tuple(x) if type(x) == list else x\n",
    "            value_counts = df[col].apply(lists_to_tuples).value_counts(dropna=False)\n",
    "            num_values = len(value_counts)\n",
    "            if num_values == 1:\n",
    "                df.drop(columns=[col],inplace=True)\n",
    "    drop_col(movies_df)\n",
    "    movies_df = movies_df.loc[:, ['imdb_id','id','title_kaggle','original_title','tagline','belongs_to_collection','url','imdb_link',\n",
    "                       'runtime','budget_kaggle','revenue','release_date_kaggle','popularity','vote_average','vote_count',\n",
    "                       'genres','original_language','overview','spoken_languages','Country',\n",
    "                       'production_companies','production_countries','Distributor',\n",
    "                       'Producer(s)','Director','Starring','Cinematography','Editor(s)','Writer(s)','Composer(s)','Based on'\n",
    "                      ]]\n",
    "     # Rename columns\n",
    "    movies_df.rename({'id':'kaggle_id',\n",
    "                  'title_kaggle':'title',\n",
    "                  'url':'wikipedia_url',\n",
    "                  'budget_kaggle':'budget',\n",
    "                  'release_date_kaggle':'release_date',\n",
    "                  'Country':'country',\n",
    "                  'Distributor':'distributor',\n",
    "                  'Producer(s)':'producers',\n",
    "                  'Director':'director',\n",
    "                  'Starring':'starring',\n",
    "                  'Cinematography':'cinematography',\n",
    "                  'Editor(s)':'editors',\n",
    "                  'Writer(s)':'writers',\n",
    "                  'Composer(s)':'composers',\n",
    "                  'Based on':'based_on'\n",
    "                 }, axis='columns', inplace=True)            \n",
    "    print('merged wiki with kaggle')\n",
    "    # Merge with rating \n",
    "    movies_with_ratings_df = pd.merge(movies_df, rating_counts, left_on='kaggle_id', right_index=True, how='left')\n",
    "    movies_with_ratings_df[rating_counts.columns] = movies_with_ratings_df[rating_counts.columns].fillna(0)\n",
    "    print('merged with rating')\n",
    "    \n",
    "    # Load movies_df to sql\n",
    "    db_string = f\"postgres://postgres:{db_password}@127.0.0.1:5432/movie_data\"\n",
    "    engine = create_engine(db_string)\n",
    "    try: \n",
    "        movies_df.to_sql(name='movies', con=engine)\n",
    "        print('Upload movies_df to sql')\n",
    "    except ValueError: \n",
    "            movies_df.to_sql(name='movies', con=engine, if_exists='append')\n",
    "            print('Upload movies_df to sql')\n",
    "    except:   \n",
    "        print('failed to upload') \n",
    "        pass     \n",
    "    # Load df with ratings to sql\n",
    "    rows_imported = 0\n",
    "    start_time = time.time()\n",
    "    for data in pd.read_csv(f'{file_dir}/ratings.csv', chunksize=1000000):\n",
    "        print(f'importing rows {rows_imported} to {rows_imported + len(data)}...', end='')\n",
    "        try: \n",
    "            data.to_sql(name='ratings', con=engine, if_exists='append')\n",
    "            rows_imported += len(data)\n",
    "            print(f'Done.{time.time() - start_time} total seconds elapsed')\n",
    "        except ValueError:\n",
    "            print('failed to upload') \n",
    "            pass \n",
    "    print('Upload movies_with_ratings to sql')\n",
    "    #return(movies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kaggle data converted\n",
      "rating data converted\n",
      "wiki data converted\n",
      "wiki data transformed\n",
      "kaggle data transformed\n",
      "rating data transformed\n",
      "merged wiki with kaggle\n",
      "merged with rating\n",
      "Upload movies_df to sql\n",
      "importing rows 0 to 1000000...Done.267.135372877121 total seconds elapsed\n",
      "importing rows 1000000 to 2000000...Done.504.3378608226776 total seconds elapsed\n",
      "importing rows 2000000 to 3000000...Done.753.531227350235 total seconds elapsed\n",
      "importing rows 3000000 to 4000000...Done.1001.0689928531647 total seconds elapsed\n",
      "importing rows 4000000 to 5000000...Done.1237.8944487571716 total seconds elapsed\n",
      "importing rows 5000000 to 6000000...Done.1452.0176339149475 total seconds elapsed\n",
      "importing rows 6000000 to 7000000...Done.1661.0025672912598 total seconds elapsed\n",
      "importing rows 7000000 to 8000000...Done.1876.6237833499908 total seconds elapsed\n",
      "importing rows 8000000 to 9000000...Done.2087.4797134399414 total seconds elapsed\n",
      "importing rows 9000000 to 10000000...Done.2301.4453315734863 total seconds elapsed\n",
      "importing rows 10000000 to 11000000...Done.24094.895613193512 total seconds elapsed\n",
      "importing rows 11000000 to 12000000...Done.24289.698532104492 total seconds elapsed\n",
      "importing rows 12000000 to 13000000...Done.24483.38700389862 total seconds elapsed\n",
      "importing rows 13000000 to 14000000...Done.24679.900269031525 total seconds elapsed\n",
      "importing rows 14000000 to 15000000...Done.24899.807022809982 total seconds elapsed\n",
      "importing rows 15000000 to 16000000...Done.25181.904671907425 total seconds elapsed\n",
      "importing rows 16000000 to 17000000...Done.25485.36102080345 total seconds elapsed\n",
      "importing rows 17000000 to 18000000...Done.25772.405381917953 total seconds elapsed\n",
      "importing rows 18000000 to 19000000...Done.26088.83337855339 total seconds elapsed\n",
      "importing rows 19000000 to 20000000...Done.26362.167637348175 total seconds elapsed\n",
      "importing rows 20000000 to 21000000...Done.26702.561893939972 total seconds elapsed\n",
      "importing rows 21000000 to 22000000...Done.27005.927590608597 total seconds elapsed\n",
      "importing rows 22000000 to 23000000...Done.27319.293232679367 total seconds elapsed\n",
      "importing rows 23000000 to 24000000...Done.27635.568273067474 total seconds elapsed\n",
      "importing rows 24000000 to 25000000...Done.27961.08540225029 total seconds elapsed\n",
      "importing rows 25000000 to 26000000...Done.28360.65242958069 total seconds elapsed\n",
      "importing rows 26000000 to 26024289...Done.28365.365813732147 total seconds elapsed\n",
      "Upload movies_with_ratings to sql\n"
     ]
    }
   ],
   "source": [
    "etl_auto('wikipedia_movies','movies_metadata','ratings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
